<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="# Agro Gemini API&#10;&#10;Una API en FastAPI que envuelve Google Gemini con un prompt de experto en agricultura para entregar recomendaciones prácticas basadas en los datos proporcionados.&#10;&#10;## Requisitos&#10;- Python 3.10+&#10;- Una clave de API de Gemini (opcional para modo demo). Crea una en https://ai.google.dev/ si deseas respuestas reales.&#10;&#10;## Configuración (Windows PowerShell)&#10;&#10;```powershell&#10;# 1) Crear entorno virtual&#10;py -m venv .venv&#10;.\.venv\Scripts\Activate.ps1&#10;&#10;# 2) Instalar dependencias&#10;pip install -r requirements.txt&#10;&#10;# 3) Configurar variables de entorno&#10;Copy-Item .env.example .env&#10;# Edita .env y coloca tu clave en GEMINI_API_KEY. Deja MOCK_MODE=true si quieres modo demo.&#10;&#10;# 4) Iniciar servidor&#10;uvicorn app.main:app --reload --port 8000&#10;```&#10;&#10;Abre http://127.0.0.1:8000/docs para probar desde Swagger UI.&#10;&#10;## Endpoints&#10;- GET `/health` -&gt; Estado del servicio, modelo y si está en modo demo.&#10;- POST `/v1/agro/ask` -&gt; Pregunta con contexto agrícola y datos. Responde con recomendaciones.&#10; - POST `/v1/agro/ask` -&gt; Pregunta con contexto agrícola y datos. Responde con recomendaciones.&#10;&#10;### Parámetros del body&#10;- question (str, requerido)&#10;- crop (str, opcional)&#10;- temperature (float, opcional)&#10;- safe_mode (bool, opcional, por defecto true): fuerza redacción educativa no prescriptiva y un reintento seguro si la respuesta es bloqueada por políticas.&#10;- length (&quot;short&quot; | &quot;medium&quot;, opcional): controla la concisión de la salida. &quot;short&quot; produce 3-4 bullets compactos; &quot;medium&quot; entrega un desarrollo breve con bullets concisos.&#10;&#10;### Ejemplo (PowerShell)&#10;```powershell&#10;$body = @{&#10;  question = &quot;Tengo maíz en V6 con hojas amarillas en bordes. ¿Qué hago?&quot;&#10;  crop = &quot;maíz&quot;&#10;  temperature = 28&#10;} | ConvertTo-Json -Depth 4&#10;&#10;Invoke-RestMethod -Uri &quot;http://127.0.0.1:8000/v1/agro/ask&quot; -Method Post -Body $body -ContentType &quot;application/json&quot;&#10;```&#10;&#10;Ejemplo con salida más concisa (length=&quot;short&quot;):&#10;```powershell&#10;$body = @{&#10;  question = &quot;Como ejemplo teórico, pautas generales de riego para maíz en etapa vegetativa.&quot;&#10;  crop = &quot;maíz&quot;&#10;  temperature = 25&#10;  length = &quot;short&quot;&#10;} | ConvertTo-Json -Depth 4&#10;&#10;Invoke-RestMethod -Uri &quot;http://127.0.0.1:8000/v1/agro/ask&quot; -Method Post -Body $body -ContentType &quot;application/json&quot;&#10;```&#10;&#10;## Notas&#10;- En modo demo (MOCK_MODE=true o sin GEMINI_API_KEY), la API devuelve una respuesta simulada útil para flujos y pruebas.&#10;- Para respuestas reales, coloca tu clave en `.env` y establece `MOCK_MODE=false`.&#10;- Ajusta el modelo con `MODEL` (por defecto `gemini-1.5-pro-latest`).&#10; - Ajusta el modelo con `MODEL` (por defecto `gemini-1.5-pro-latest`). Recomendado: `gemini-2.5-flash` por velocidad.&#10;&#10;## Estructura&#10;```&#10;app/&#10;  main.py&#10;  config.py&#10;  routes/agro.py&#10;  services/gemini_client.py&#10;  prompts/agriculture_system_prompt.md&#10;  schemas/&#10;    requests.py&#10;    responses.py&#10;scripts/smoke_test.py&#10;```&#10;&#10;## Problemas comunes&#10;- SSL/Firewall: si tienes bloqueos de red, las llamadas al modelo podrían fallar. Prueba primero en modo demo.&#10;- Timeouts: incrementa `TIMEOUT_S` en `.env` si tu red es lenta.&#10;- Longitud: si tu `question` es muy larga, se rechazará con 400.&#10;" />
              <option name="updatedContent" value="# Agro Gemini API&#10;&#10;Una API en FastAPI que envuelve Google Gemini con un prompt de experto en agricultura para entregar recomendaciones prácticas basadas en los datos proporcionados.&#10;&#10;## Requisitos&#10;- Python 3.10+&#10;- Una clave de API de Gemini (opcional para modo demo). Crea una en https://ai.google.dev/ si deseas respuestas reales.&#10;&#10;## Configuración (Windows PowerShell)&#10;&#10;```powershell&#10;# 1) Crear entorno virtual&#10;py -m venv .venv&#10;.\.venv\Scripts\Activate.ps1&#10;&#10;# 2) Instalar dependencias&#10;pip install -r requirements.txt&#10;&#10;# 3) Configurar variables de entorno&#10;Copy-Item .env.example .env&#10;# Edita .env y coloca tu clave en GEMINI_API_KEY. Deja MOCK_MODE=true si quieres modo demo.&#10;&#10;# 4) Iniciar servidor&#10;uvicorn app.main:app --reload --port 8000&#10;```&#10;&#10;Abre http://127.0.0.1:8000/docs para probar desde Swagger UI.&#10;&#10;## Despliegue en Vercel&#10;Este proyecto está listo para ser desplegado como Función Serverless de Python en Vercel.&#10;&#10;- Estructura: el entrypoint para Vercel es `api/index.py`, que expone la app ASGI.&#10;- Configuración de rutas: `vercel.json` redirige todo a `api/index.py`.&#10;- Requisitos en Vercel: Vercel instala automáticamente las dependencias detectando `requirements.txt`.&#10;&#10;Pasos:&#10;1) Inicia sesión en Vercel (CLI) y despliega.&#10;&#10;```cmd&#10;vercel login&#10;vercel --prod&#10;```&#10;&#10;2) Variables de entorno en Vercel (Dashboard o CLI):&#10;- `GEMINI_API_KEY`: tu clave de Google AI Studio (opcional si usas demo).&#10;- `MODEL` (opcional): por defecto `gemini-1.5-pro-latest` (recomendado: `gemini-2.5-flash`).&#10;- `MOCK_MODE`: `true` (demo) o `false` (real).&#10;- `LOG_LEVEL` (opcional): `INFO` por defecto.&#10;- `TIMEOUT_S` (opcional): `30` por defecto.&#10;- `MAX_INPUT_CHARS` (opcional): `12000` por defecto.&#10;&#10;3) Probar endpoints desplegados (reemplaza la URL):&#10;```powershell&#10;$base = &quot;https://tu-deploy.vercel.app&quot;&#10;Invoke-RestMethod -Uri &quot;$base/health&quot; -Method Get&#10;&#10;$body = @{ question = &quot;Tengo maíz en V6 con hojas amarillas en bordes. ¿Qué hago?&quot;; crop = &quot;maíz&quot;; temperature = 28 } | ConvertTo-Json -Depth 4&#10;Invoke-RestMethod -Uri &quot;$base/v1/agro/ask&quot; -Method Post -Body $body -ContentType &quot;application/json&quot;&#10;```&#10;&#10;### Salir del modo DEMO en Vercel&#10;Si recibes respuestas con `[MODO DEMO]`, significa que el servicio está en modo simulado. Para respuestas reales, configura:&#10;&#10;- Define `GEMINI_API_KEY` en el entorno (Producción) de Vercel.&#10;- Establece `MOCK_MODE=false` en el entorno (Producción).&#10;- Redeploya el proyecto para aplicar los cambios.&#10;&#10;CLI (interactivo) en Windows:&#10;```cmd&#10;vercel env add GEMINI_API_KEY production&#10;# Pega tu clave y confirma&#10;vercel env add MOCK_MODE production&#10;# Escribe: false&#10;&#10;# Después de actualizar variables, redeploya&#10;vercel --prod&#10;```&#10;&#10;Verificación rápida:&#10;- `GET /health` debe devolver `&quot;mock_mode&quot;: false` y el nombre de modelo.&#10;- `POST /v1/agro/ask` ya no incluirá la etiqueta `[MODO DEMO]`.&#10;&#10;Notas para serverless:&#10;- El archivo de prompt está embebido con un fallback si no se puede leer desde disco (útil en entornos serverless).&#10;- Si no defines `GEMINI_API_KEY` o hay error al configurar el cliente, la API cae automáticamente en modo demo.&#10;&#10;## Endpoints&#10;- GET `/health` -&gt; Estado del servicio, modelo y si está en modo demo.&#10;- POST `/v1/agro/ask` -&gt; Pregunta con contexto agrícola y datos. Responde con recomendaciones.&#10;&#10;### Parámetros del body&#10;- question (str, requerido)&#10;- crop (str, opcional)&#10;- temperature (float, opcional)&#10;- safe_mode (bool, opcional, por defecto true): fuerza redacción educativa no prescriptiva y un reintento seguro si la respuesta es bloqueada por políticas.&#10;- length (&quot;short&quot; | &quot;medium&quot;, opcional): controla la concisión de la salida. &quot;short&quot; produce 3-4 bullets compactos; &quot;medium&quot; entrega un desarrollo breve con bullets concisos.&#10;&#10;### Ejemplo (PowerShell)&#10;```powershell&#10;$body = @{&#10;  question = &quot;Tengo maíz en V6 con hojas amarillas en bordes. ¿Qué hago?&quot;&#10;  crop = &quot;maíz&quot;&#10;  temperature = 28&#10;} | ConvertTo-Json -Depth 4&#10;&#10;Invoke-RestMethod -Uri &quot;http://127.0.0.1:8000/v1/agro/ask&quot; -Method Post -Body $body -ContentType &quot;application/json&quot;&#10;```&#10;&#10;Ejemplo con salida más concisa (length=&quot;short&quot;):&#10;```powershell&#10;$body = @{&#10;  question = &quot;Como ejemplo teórico, pautas generales de riego para maíz en etapa vegetativa.&quot;&#10;  crop = &quot;maíz&quot;&#10;  temperature = 25&#10;  length = &quot;short&quot;&#10;} | ConvertTo-Json -Depth 4&#10;&#10;Invoke-RestMethod -Uri &quot;http://127.0.0.1:8000/v1/agro/ask&quot; -Method Post -Body $body -ContentType &quot;application/json&quot;&#10;```&#10;&#10;## Notas&#10;- En modo demo (MOCK_MODE=true o sin GEMINI_API_KEY), la API devuelve una respuesta simulada útil para flujos y pruebas.&#10;- Para respuestas reales, coloca tu clave en `.env` y establece `MOCK_MODE=false`.&#10;- Ajusta el modelo con `MODEL` (por defecto `gemini-1.5-pro-latest`). Recomendado: `gemini-2.5-flash` por velocidad.&#10;&#10;## Estructura&#10;```&#10;app/&#10;  main.py&#10;  config.py&#10;  routes/agro.py&#10;  services/gemini_client.py&#10;  prompts/agriculture_system_prompt.md&#10;  schemas/&#10;    requests.py&#10;    responses.py&#10;api/&#10;  index.py&#10;vercel.json&#10;scripts/smoke_test.py&#10;```&#10;&#10;## Problemas comunes&#10;- SSL/Firewall: si tienes bloqueos de red, las llamadas al modelo podrían fallar. Prueba primero en modo demo.&#10;- Timeouts: incrementa `TIMEOUT_S` en `.env` si tu red es lenta.&#10;- Longitud: si tu `question` es muy larga, se rechazará con 400." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/api/index.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/api/index.py" />
              <option name="updatedContent" value="# Vercel Serverless Function entrypoint for FastAPI (ASGI)&#10;# Exposes the FastAPI `app` at the module level so Vercel can serve it.&#10;&#10;from app.main import app as _fastapi_app&#10;&#10;# Vercel expects a variable named `app` which is an ASGI or WSGI application.&#10;app = _fastapi_app&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/app/services/gemini_client.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/app/services/gemini_client.py" />
              <option name="originalContent" value="from __future__ import annotations&#10;&#10;import json&#10;from pathlib import Path&#10;from typing import Any, Dict, List, Optional&#10;&#10;from tenacity import retry, stop_after_attempt, wait_exponential&#10;&#10;from app.config import get_settings&#10;from app.schemas.requests import AskRequest&#10;from app.schemas.responses import AskResponse&#10;from app.utils.logger import get_logger&#10;from app.utils.sanitize import sanitize_question, sanitize_data_preview&#10;&#10;logger = get_logger(&quot;agro.gemini&quot;)&#10;&#10;&#10;class GeminiClient:&#10;    def __init__(self, prompt_path: Path):&#10;        self.settings = get_settings()&#10;        self.prompt_text = prompt_path.read_text(encoding=&quot;utf-8&quot;)&#10;        self._model = None&#10;        self._configured = False&#10;&#10;    def _configure(self):&#10;        if self._configured:&#10;            return&#10;        if not self.settings.mock_mode and not self.settings.gemini_api_key:&#10;            logger.warning(&quot;No GEMINI_API_KEY provided. Falling back to mock mode.&quot;)&#10;            self.settings.mock_mode = True&#10;            return&#10;        try:&#10;            import google.generativeai as genai&#10;&#10;            genai.configure(api_key=self.settings.gemini_api_key)&#10;            # Determine best available model for generateContent&#10;            requested = (self.settings.gemini_model or &quot;&quot;).strip()&#10;            normalized = requested.replace(&quot;-latest&quot;, &quot;&quot;) if requested.endswith(&quot;-latest&quot;) else requested&#10;            # Prefer 2.5 family if available&#10;            preferences = [&#10;                normalized,&#10;                &quot;gemini-2.5-pro&quot;,&#10;                &quot;gemini-2.5-flash&quot;,&#10;                &quot;gemini-2.0-flash&quot;,&#10;                &quot;gemini-1.5-pro&quot;,&#10;                &quot;gemini-1.5-flash&quot;,&#10;            ]&#10;&#10;            try:&#10;                models = list(genai.list_models())&#10;                avail = {&#10;                    getattr(m, &quot;name&quot;, &quot;&quot;).split(&quot;/&quot;)[-1]&#10;                    for m in models&#10;                    if &quot;supported_generation_methods&quot; in dir(m)&#10;                    and &quot;generateContent&quot; in getattr(m, &quot;supported_generation_methods&quot;, [])&#10;                }&#10;            except Exception:&#10;                avail = set()&#10;&#10;            candidates = [m for m in preferences if m] or [&quot;gemini-2.5-flash&quot;]&#10;            if avail:&#10;                candidates = [m for m in candidates if m in avail] or list(avail)&#10;&#10;            last_err = None&#10;            for m in candidates:&#10;                try:&#10;                    self._model = genai.GenerativeModel(&#10;                        model_name=m,&#10;                        system_instruction=self.prompt_text,&#10;                    )&#10;                    self.settings.gemini_model = m&#10;                    break&#10;                except Exception as e:  # try next candidate&#10;                    last_err = e&#10;                    continue&#10;            if self._model is None:&#10;                raise last_err or RuntimeError(&quot;No se pudo configurar el modelo de Gemini.&quot;)&#10;            self._genai = genai&#10;            self._configured = True&#10;            logger.info(&quot;Gemini client configured with model %s&quot;, self.settings.gemini_model)&#10;        except Exception as e:&#10;            logger.exception(&quot;Failed to configure Gemini: %s&quot;, e)&#10;            self.settings.mock_mode = True&#10;&#10;    def _compose_user_prompt(self, req: AskRequest) -&gt; str:&#10;        parts: List[str] = []&#10;        # Educational, non-prescriptive framing to reduce safety blocks&#10;        parts.append(&#10;            &quot;Contexto educativo: Esta consulta es únicamente informativa y de ejemplo teórico para agricultura. &quot;&#10;            &quot;No contiene datos personales ni requiere instrucciones operativas. Evita nombres comerciales o marcas; no incluyas cantidades numéricas, calendarios ni instrucciones paso a paso. &quot;&#10;            &quot;Responde en tono no prescriptivo (&quot;&quot;podría&quot;&quot;, &quot;&quot;en general&quot;&quot;, &quot;&quot;como referencia&quot;&quot;) y con foco en buenas prácticas.&quot;&#10;        )&#10;        if req.crop:&#10;            parts.append(f&quot;Cultivo: {req.crop}&quot;)&#10;        safe_q = sanitize_question(req.question, max_len=min(800, self.settings.max_input_chars))&#10;        parts.append(f&quot;Pregunta: {safe_q}&quot;)&#10;        if getattr(req, &quot;temperature&quot;, None) is not None:&#10;            parts.append(f&quot;Temperatura (°C): {req.temperature}&quot;)&#10;        # Gentle instruction on output style&#10;        parts.append(&#10;            &quot;Formato de salida: \n&quot;&#10;            &quot;- 1) Resumen educativo breve\n&quot;&#10;            &quot;- 2) Pautas generales (bullets, no prescripciones)\n&quot;&#10;            &quot;- 3) Parámetros de referencia (rangos típicos)\n&quot;&#10;            &quot;- 4) Monitoreo sugerido\n&quot;&#10;            &quot;- 5) Riesgos y mitigaciones generales\n&quot;&#10;            &quot;- 6) Datos extra útiles (si aplican)&quot;&#10;        )&#10;        # Length guidance (parametrized)&#10;        length = getattr(req, &quot;length&quot;, None) or &quot;medium&quot;&#10;        if length == &quot;short&quot;:&#10;            parts.append(&#10;                &quot;Longitud sugerida: 3–5 bullets concisos (~150–220 palabras). &quot;&#10;                &quot;Evita pasos operativos, imperativos o detalles numéricos.&quot;&#10;            )&#10;        else:&#10;            parts.append(&#10;                &quot;Mantén la respuesta concisa (≈ 200–350 palabras) y enfocada en bullets; evita redundancias.&quot;&#10;            )&#10;        return &quot;\n\n&quot;.join(parts)&#10;&#10;    def _build_generation_config(self, *, length: Optional[str] = None) -&gt; Dict[str, Any]:&#10;        max_tokens = 900&#10;        temperature = 0.2&#10;        top_p = 0.9&#10;        if length == &quot;short&quot;:&#10;            max_tokens = 520&#10;            temperature = 0.1&#10;            top_p = 0.7&#10;        elif length == &quot;medium&quot; or length is None:&#10;            max_tokens = 900&#10;        return {&#10;            &quot;temperature&quot;: temperature,&#10;            &quot;top_p&quot;: top_p,&#10;            &quot;top_k&quot;: 40,&#10;            &quot;max_output_tokens&quot;: max_tokens,&#10;        }&#10;&#10;    def _safety_settings(self) -&gt; List[Dict[str, str]]:&#10;        # Relax safety just to block only high-likelihood harmful content, reducing false positives&#10;        return [&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_DANGEROUS_CONTENT&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_HARASSMENT&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_HATE_SPEECH&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_SEXUALLY_EXPLICIT&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#10;        ]&#10;&#10;    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.5, min=0.5, max=4), reraise=True)&#10;    def _call_gemini(self, user_prompt: str, *, allow_reframe: bool = True, length: Optional[str] = None) -&gt; AskResponse:&#10;        config = self._build_generation_config(length=length)&#10;        try:&#10;            response = self._model.generate_content(&#10;                user_prompt,&#10;                generation_config=config,&#10;                safety_settings=self._safety_settings(),&#10;            )&#10;        except Exception as e:&#10;            msg = str(e).lower()&#10;            # Fallback to a widely available model if the requested one isn't supported&#10;            if &quot;404&quot; in msg or &quot;not found&quot; in msg or &quot;unsupported&quot; in msg:&#10;                try:&#10;                    import google.generativeai as genai&#10;&#10;                    fallback = &quot;gemini-1.5-flash&quot;&#10;                    logger.info(&quot;Falling back to model %s due to availability error&quot;, fallback)&#10;                    self._model = genai.GenerativeModel(&#10;                        model_name=fallback,&#10;                        system_instruction=self.prompt_text,&#10;                    )&#10;                    self.settings.gemini_model = fallback&#10;                    response = self._model.generate_content(&#10;                        user_prompt,&#10;                        generation_config=config,&#10;                        safety_settings=self._safety_settings(),&#10;                    )&#10;                except Exception as e2:&#10;                    logger.exception(&quot;Gemini fallback call failed: %s&quot;, e2)&#10;                    raise e2&#10;            else:&#10;                logger.exception(&quot;Gemini call failed: %s&quot;, e)&#10;                raise&#10;&#10;        # Extract answer text robustly without accessing response.text property&#10;        answer = &quot;&quot;&#10;        finish_reason = None&#10;        try:&#10;            candidates = getattr(response, &quot;candidates&quot;, None) or []&#10;            if candidates:&#10;                first = candidates[0]&#10;                content = getattr(first, &quot;content&quot;, None)&#10;                parts = getattr(content, &quot;parts&quot;, []) if content else []&#10;                texts = []&#10;                for p in parts:&#10;                    t = getattr(p, &quot;text&quot;, None)&#10;                    if isinstance(t, str) and t:&#10;                        texts.append(t)&#10;                    elif isinstance(p, str) and p:&#10;                        texts.append(p)&#10;                answer = &quot;\n&quot;.join(texts)&#10;                finish_reason = getattr(first, &quot;finish_reason&quot;, None)&#10;                if not answer and finish_reason not in (None, 0):&#10;                    answer = (&#10;                        &quot;La respuesta fue bloqueada por las políticas de seguridad del modelo. &quot;&#10;                        &quot;Intenta reformular la pregunta con términos neutros y sin información sensible.&quot;&#10;                    )&#10;        except Exception:&#10;            pass&#10;        usage_md = getattr(response, &quot;usage_metadata&quot;, None)&#10;        usage = None&#10;        if usage_md:&#10;            # Best-effort conversion to dict&#10;            usage = {&#10;                k: getattr(usage_md, k)&#10;                for k in dir(usage_md)&#10;                if not k.startswith(&quot;_&quot;) and not callable(getattr(usage_md, k))&#10;            }&#10;        # If blocked or empty, try a single educational reframe to reduce safety triggers&#10;        if allow_reframe and (not answer or &quot;fue bloqueada&quot; in answer.lower() or finish_reason not in (None, 0)):&#10;            try:&#10;                re_user_prompt = (&#10;                    user_prompt&#10;                    + &quot;\n\nReformulación: Proporciona únicamente un resumen educativo general de alto nivel. &quot;&#10;                      &quot;Evita pasos operativos, cantidades, dosis, calendarios o imperativos. &quot;&#10;                      &quot;No incluyas productos, marcas ni instrucciones de ‘cómo hacer’. &quot;&#10;                      &quot;En su lugar, resume factores a considerar, buenas prácticas generales y señales de monitoreo, usando lenguaje condicional. &quot;&#10;                      &quot;No apliques límites de longitud estrictos; prioriza neutralidad y claridad (≈150–300 palabras en bullets).&quot;&#10;                )&#10;                response2 = self._model.generate_content(&#10;                    re_user_prompt,&#10;                    generation_config={**config, &quot;temperature&quot;: 0.1, &quot;top_p&quot;: 0.7},&#10;                    safety_settings=self._safety_settings(),&#10;                )&#10;                # Extract again&#10;                answer2 = &quot;&quot;&#10;                try:&#10;                    candidates2 = getattr(response2, &quot;candidates&quot;, None) or []&#10;                    if candidates2:&#10;                        first2 = candidates2[0]&#10;                        content2 = getattr(first2, &quot;content&quot;, None)&#10;                        parts2 = getattr(content2, &quot;parts&quot;, []) if content2 else []&#10;                        texts2 = []&#10;                        for p in parts2:&#10;                            t2 = getattr(p, &quot;text&quot;, None)&#10;                            if isinstance(t2, str) and t2:&#10;                                texts2.append(t2)&#10;                            elif isinstance(p, str) and p:&#10;                                texts2.append(p)&#10;                        answer2 = &quot;\n&quot;.join(texts2)&#10;                except Exception:&#10;                    pass&#10;                if answer2:&#10;                    answer = answer2&#10;            except Exception:&#10;                pass&#10;&#10;            # Second, more generic reframe if still empty/blocked&#10;            if not answer:&#10;                try:&#10;                    re_user_prompt2 = (&#10;                        &quot;Finalidad educativa: Ofrece un panorama general sobre manejo del agua en cultivos en términos amplios y neutros. &quot;&#10;                        &quot;Evita pasos operativos, cantidades, dosis, calendarios, marcas o productos. &quot;&#10;                        &quot;Usa bullets y lenguaje condicional para describir factores a considerar (clima, suelo, fenología, monitoreo), sin recomendaciones prescriptivas.&quot;&#10;                    )&#10;                    response3 = self._model.generate_content(&#10;                        re_user_prompt2,&#10;                        generation_config={**config, &quot;temperature&quot;: 0.1, &quot;top_p&quot;: 0.6},&#10;                        safety_settings=self._safety_settings(),&#10;                    )&#10;                    answer3 = &quot;&quot;&#10;                    try:&#10;                        candidates3 = getattr(response3, &quot;candidates&quot;, None) or []&#10;                        if candidates3:&#10;                            first3 = candidates3[0]&#10;                            content3 = getattr(first3, &quot;content&quot;, None)&#10;                            parts3 = getattr(content3, &quot;parts&quot;, []) if content3 else []&#10;                            texts3 = []&#10;                            for p in parts3:&#10;                                t3 = getattr(p, &quot;text&quot;, None)&#10;                                if isinstance(t3, str) and t3:&#10;                                    texts3.append(t3)&#10;                                elif isinstance(p, str) and p:&#10;                                    texts3.append(p)&#10;                            answer3 = &quot;\n&quot;.join(texts3)&#10;                    except Exception:&#10;                        pass&#10;                    if answer3:&#10;                        answer = answer3&#10;                except Exception:&#10;                    pass&#10;&#10;            # Third fallback: if requested short still fails, try concise-medium guidance&#10;            if not answer and (length == &quot;short&quot;):&#10;                try:&#10;                    re_user_prompt3 = (&#10;                        user_prompt&#10;                        + &quot;\n\nAjuste de formato: Responde de forma concisa (≈ 200–300 palabras) en bullets educativos. &quot;&#10;                          &quot;Evita pasos, cantidades numéricas, calendarios, marcas o productos. Usa lenguaje condicional.&quot;&#10;                    )&#10;                    response4 = self._model.generate_content(&#10;                        re_user_prompt3,&#10;                        generation_config=self._build_generation_config(length=&quot;medium&quot;),&#10;                        safety_settings=self._safety_settings(),&#10;                    )&#10;                    answer4 = &quot;&quot;&#10;                    try:&#10;                        candidates4 = getattr(response4, &quot;candidates&quot;, None) or []&#10;                        if candidates4:&#10;                            first4 = candidates4[0]&#10;                            content4 = getattr(first4, &quot;content&quot;, None)&#10;                            parts4 = getattr(content4, &quot;parts&quot;, []) if content4 else []&#10;                            texts4 = []&#10;                            for p in parts4:&#10;                                t4 = getattr(p, &quot;text&quot;, None)&#10;                                if isinstance(t4, str) and t4:&#10;                                    texts4.append(t4)&#10;                                elif isinstance(p, str) and p:&#10;                                    texts4.append(p)&#10;                            answer4 = &quot;\n&quot;.join(texts4)&#10;                    except Exception:&#10;                        pass&#10;                    if answer4:&#10;                        answer = answer4&#10;                except Exception:&#10;                    pass&#10;&#10;        return AskResponse(answer=answer.strip(), model=self.settings.gemini_model, usage=usage, tips=None)&#10;&#10;    def ask(self, req: AskRequest) -&gt; AskResponse:&#10;        if len(req.question) &gt; self.settings.max_input_chars:&#10;            raise ValueError(&quot;La pregunta es demasiado larga. Reduce el tamaño del texto.&quot;)&#10;&#10;        user_prompt = self._compose_user_prompt(req)&#10;        length = getattr(req, &quot;length&quot;, None) or &quot;medium&quot;&#10;&#10;        # If already in mock mode, return a deterministic demo response&#10;        if self.settings.mock_mode:&#10;            logger.debug(&quot;Using mock mode for response.&quot;)&#10;            tips = [&#10;                &quot;Incluye datos de suelo (pH, CE, % humedad) y clima (ET0, precipitación).&quot;,&#10;                &quot;Especifica el estado fenológico del cultivo para recomendaciones más precisas.&quot;,&#10;            ]&#10;            answer = (&#10;                &quot;[MODO DEMO] Recomendación preliminar para agricultura basada en la información disponible. &quot;&#10;                &quot;Agrega tu GEMINI_API_KEY en .env para respuestas reales.\n\n&quot;&#10;                f&quot;Resumen: {req.question[:180]}...\n\n&quot;&#10;                &quot;Siguiente paso: proporciona datos de suelo y clima para ajustar dosis y calendario.&quot;&#10;            )&#10;            return AskResponse(answer=answer, model=self.settings.gemini_model, usage=None, tips=tips)&#10;&#10;        # Ensure client is configured; on failure, configuration can toggle mock_mode&#10;        self._configure()&#10;        if self.settings.mock_mode:&#10;            # If configuration failed and switched to mock mode, return mock response now&#10;            logger.debug(&quot;Configuration switched to mock mode; returning demo response.&quot;)&#10;            tips = [&#10;                &quot;Incluye datos de suelo (pH, CE, % humedad) y clima (ET0, precipitación).&quot;,&#10;                &quot;Especifica el estado fenológico del cultivo para recomendaciones más precisas.&quot;,&#10;            ]&#10;            answer = (&#10;                &quot;[MODO DEMO] Recomendación preliminar para agricultura basada en la información disponible. &quot;&#10;                &quot;Agrega tu GEMINI_API_KEY en .env para respuestas reales.\n\n&quot;&#10;                f&quot;Resumen: {req.question[:180]}...\n\n&quot;&#10;                &quot;Siguiente paso: proporciona datos de suelo y clima para ajustar dosis y calendario.&quot;&#10;            )&#10;            return AskResponse(answer=answer, model=self.settings.gemini_model, usage=None, tips=tips)&#10;&#10;        return self._call_gemini(user_prompt, allow_reframe=bool(getattr(req, &quot;safe_mode&quot;, True)), length=length)&#10;" />
              <option name="updatedContent" value="from __future__ import annotations&#13;&#10;&#13;&#10;import json&#13;&#10;from pathlib import Path&#13;&#10;from typing import Any, Dict, List, Optional&#13;&#10;&#13;&#10;from tenacity import retry, stop_after_attempt, wait_exponential&#13;&#10;&#13;&#10;from app.config import get_settings&#13;&#10;from app.schemas.requests import AskRequest&#13;&#10;from app.schemas.responses import AskResponse&#13;&#10;from app.utils.logger import get_logger&#13;&#10;from app.utils.sanitize import sanitize_question, sanitize_data_preview&#13;&#10;&#13;&#10;logger = get_logger(&quot;agro.gemini&quot;)&#13;&#10;&#13;&#10;&#13;&#10;class GeminiClient:&#13;&#10;    def __init__(self, prompt_path: Path):&#13;&#10;        self.settings = get_settings()&#13;&#10;        # Carga robusta del prompt para entornos serverless&#13;&#10;        default_prompt = (&#13;&#10;            &quot;Eres un asistente educativo en agricultura. Brindas orientación general, no prescriptiva, &quot;&#13;&#10;            &quot;evitando marcas, dosis exactas y pasos operativos. Te enfocas en buenas prácticas, factores a considerar, &quot;&#13;&#10;            &quot;rangos típicos y sugerencias de monitoreo. Respondes en español de forma clara y concisa.&quot;&#13;&#10;        )&#13;&#10;        try:&#13;&#10;            self.prompt_text = prompt_path.read_text(encoding=&quot;utf-8&quot;)&#13;&#10;        except Exception:&#13;&#10;            logger.warning(&quot;No se pudo leer el archivo de prompt en %s; usando prompt por defecto.&quot;, prompt_path)&#13;&#10;            self.prompt_text = default_prompt&#13;&#10;        self._model = None&#13;&#10;        self._configured = False&#13;&#10;&#13;&#10;    def _configure(self):&#13;&#10;        if self._configured:&#13;&#10;            return&#13;&#10;        if not self.settings.mock_mode and not self.settings.gemini_api_key:&#13;&#10;            logger.warning(&quot;No GEMINI_API_KEY provided. Falling back to mock mode.&quot;)&#13;&#10;            self.settings.mock_mode = True&#13;&#10;            return&#13;&#10;        try:&#13;&#10;            import google.generativeai as genai&#13;&#10;&#13;&#10;            genai.configure(api_key=self.settings.gemini_api_key)&#13;&#10;            # Determine best available model for generateContent&#13;&#10;            requested = (self.settings.gemini_model or &quot;&quot;).strip()&#13;&#10;            normalized = requested.replace(&quot;-latest&quot;, &quot;&quot;) if requested.endswith(&quot;-latest&quot;) else requested&#13;&#10;            # Prefer 2.5 family if available&#13;&#10;            preferences = [&#13;&#10;                normalized,&#13;&#10;                &quot;gemini-2.5-pro&quot;,&#13;&#10;                &quot;gemini-2.5-flash&quot;,&#13;&#10;                &quot;gemini-2.0-flash&quot;,&#13;&#10;                &quot;gemini-1.5-pro&quot;,&#13;&#10;                &quot;gemini-1.5-flash&quot;,&#13;&#10;            ]&#13;&#10;&#13;&#10;            try:&#13;&#10;                models = list(genai.list_models())&#13;&#10;                avail = {&#13;&#10;                    getattr(m, &quot;name&quot;, &quot;&quot;).split(&quot;/&quot;)[-1]&#13;&#10;                    for m in models&#13;&#10;                    if &quot;supported_generation_methods&quot; in dir(m)&#13;&#10;                    and &quot;generateContent&quot; in getattr(m, &quot;supported_generation_methods&quot;, [])&#13;&#10;                }&#13;&#10;            except Exception:&#13;&#10;                avail = set()&#13;&#10;&#13;&#10;            candidates = [m for m in preferences if m] or [&quot;gemini-2.5-flash&quot;]&#13;&#10;            if avail:&#13;&#10;                candidates = [m for m in candidates if m in avail] or list(avail)&#13;&#10;&#13;&#10;            last_err = None&#13;&#10;            for m in candidates:&#13;&#10;                try:&#13;&#10;                    self._model = genai.GenerativeModel(&#13;&#10;                        model_name=m,&#13;&#10;                        system_instruction=self.prompt_text,&#13;&#10;                    )&#13;&#10;                    self.settings.gemini_model = m&#13;&#10;                    break&#13;&#10;                except Exception as e:  # try next candidate&#13;&#10;                    last_err = e&#13;&#10;                    continue&#13;&#10;            if self._model is None:&#13;&#10;                raise last_err or RuntimeError(&quot;No se pudo configurar el modelo de Gemini.&quot;)&#13;&#10;            self._genai = genai&#13;&#10;            self._configured = True&#13;&#10;            logger.info(&quot;Gemini client configured with model %s&quot;, self.settings.gemini_model)&#13;&#10;        except Exception as e:&#13;&#10;            logger.exception(&quot;Failed to configure Gemini: %s&quot;, e)&#13;&#10;            self.settings.mock_mode = True&#13;&#10;&#13;&#10;    def _compose_user_prompt(self, req: AskRequest) -&gt; str:&#13;&#10;        parts: List[str] = []&#13;&#10;        # Educational, non-prescriptive framing to reduce safety blocks&#13;&#10;        parts.append(&#13;&#10;            &quot;Contexto educativo: Esta consulta es únicamente informativa y de ejemplo teórico para agricultura. &quot;&#13;&#10;            &quot;No contiene datos personales ni requiere instrucciones operativas. Evita nombres comerciales o marcas; no incluyas cantidades numéricas, calendarios ni instrucciones paso a paso. &quot;&#13;&#10;            &quot;Responde en tono no prescriptivo (&quot;&quot;podría&quot;&quot;, &quot;&quot;en general&quot;&quot;, &quot;&quot;como referencia&quot;&quot;) y con foco en buenas prácticas.&quot;&#13;&#10;        )&#13;&#10;        if req.crop:&#13;&#10;            parts.append(f&quot;Cultivo: {req.crop}&quot;)&#13;&#10;        safe_q = sanitize_question(req.question, max_len=min(800, self.settings.max_input_chars))&#13;&#10;        parts.append(f&quot;Pregunta: {safe_q}&quot;)&#13;&#10;        if getattr(req, &quot;temperature&quot;, None) is not None:&#13;&#10;            parts.append(f&quot;Temperatura (°C): {req.temperature}&quot;)&#13;&#10;        # Gentle instruction on output style&#13;&#10;        parts.append(&#13;&#10;            &quot;Formato de salida: \n&quot;&#13;&#10;            &quot;- 1) Resumen educativo breve\n&quot;&#13;&#10;            &quot;- 2) Pautas generales (bullets, no prescripciones)\n&quot;&#13;&#10;            &quot;- 3) Parámetros de referencia (rangos típicos)\n&quot;&#13;&#10;            &quot;- 4) Monitoreo sugerido\n&quot;&#13;&#10;            &quot;- 5) Riesgos y mitigaciones generales\n&quot;&#13;&#10;            &quot;- 6) Datos extra útiles (si aplican)&quot;&#13;&#10;        )&#13;&#10;        # Length guidance (parametrized)&#13;&#10;        length = getattr(req, &quot;length&quot;, None) or &quot;medium&quot;&#13;&#10;        if length == &quot;short&quot;:&#13;&#10;            parts.append(&#13;&#10;                &quot;Longitud sugerida: 3–5 bullets concisos (~150–220 palabras). &quot;&#13;&#10;                &quot;Evita pasos operativos, imperativos o detalles numéricos.&quot;&#13;&#10;            )&#13;&#10;        else:&#13;&#10;            parts.append(&#13;&#10;                &quot;Mantén la respuesta concisa (≈ 200–350 palabras) y enfocada en bullets; evita redundancias.&quot;&#13;&#10;            )&#13;&#10;        return &quot;\n\n&quot;.join(parts)&#13;&#10;&#13;&#10;    def _build_generation_config(self, *, length: Optional[str] = None) -&gt; Dict[str, Any]:&#13;&#10;        max_tokens = 900&#13;&#10;        temperature = 0.2&#13;&#10;        top_p = 0.9&#13;&#10;        if length == &quot;short&quot;:&#13;&#10;            max_tokens = 520&#13;&#10;            temperature = 0.1&#13;&#10;            top_p = 0.7&#13;&#10;        elif length == &quot;medium&quot; or length is None:&#13;&#10;            max_tokens = 900&#13;&#10;        return {&#13;&#10;            &quot;temperature&quot;: temperature,&#13;&#10;            &quot;top_p&quot;: top_p,&#13;&#10;            &quot;top_k&quot;: 40,&#13;&#10;            &quot;max_output_tokens&quot;: max_tokens,&#13;&#10;        }&#13;&#10;&#13;&#10;    def _safety_settings(self) -&gt; List[Dict[str, str]]:&#13;&#10;        # Relax safety just to block only high-likelihood harmful content, reducing false positives&#13;&#10;        return [&#13;&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_DANGEROUS_CONTENT&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#13;&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_HARASSMENT&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#13;&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_HATE_SPEECH&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#13;&#10;            {&quot;category&quot;: &quot;HARM_CATEGORY_SEXUALLY_EXPLICIT&quot;, &quot;threshold&quot;: &quot;BLOCK_ONLY_HIGH&quot;},&#13;&#10;        ]&#13;&#10;&#13;&#10;    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=0.5, min=0.5, max=4), reraise=True)&#13;&#10;    def _call_gemini(self, user_prompt: str, *, allow_reframe: bool = True, length: Optional[str] = None) -&gt; AskResponse:&#13;&#10;        config = self._build_generation_config(length=length)&#13;&#10;        try:&#13;&#10;            response = self._model.generate_content(&#13;&#10;                user_prompt,&#13;&#10;                generation_config=config,&#13;&#10;                safety_settings=self._safety_settings(),&#13;&#10;            )&#13;&#10;        except Exception as e:&#13;&#10;            msg = str(e).lower()&#13;&#10;            # Fallback to a widely available model if the requested one isn't supported&#13;&#10;            if &quot;404&quot; in msg or &quot;not found&quot; in msg or &quot;unsupported&quot; in msg:&#13;&#10;                try:&#13;&#10;                    import google.generativeai as genai&#13;&#10;&#13;&#10;                    fallback = &quot;gemini-1.5-flash&quot;&#13;&#10;                    logger.info(&quot;Falling back to model %s due to availability error&quot;, fallback)&#13;&#10;                    self._model = genai.GenerativeModel(&#13;&#10;                        model_name=fallback,&#13;&#10;                        system_instruction=self.prompt_text,&#13;&#10;                    )&#13;&#10;                    self.settings.gemini_model = fallback&#13;&#10;                    response = self._model.generate_content(&#13;&#10;                        user_prompt,&#13;&#10;                        generation_config=config,&#13;&#10;                        safety_settings=self._safety_settings(),&#13;&#10;                    )&#13;&#10;                except Exception as e2:&#13;&#10;                    logger.exception(&quot;Gemini fallback call failed: %s&quot;, e2)&#13;&#10;                    raise e2&#13;&#10;            else:&#13;&#10;                logger.exception(&quot;Gemini call failed: %s&quot;, e)&#13;&#10;                raise&#13;&#10;&#13;&#10;        # Extract answer text robustly without accessing response.text property&#13;&#10;        answer = &quot;&quot;&#13;&#10;        finish_reason = None&#13;&#10;        try:&#13;&#10;            candidates = getattr(response, &quot;candidates&quot;, None) or []&#13;&#10;            if candidates:&#13;&#10;                first = candidates[0]&#13;&#10;                content = getattr(first, &quot;content&quot;, None)&#13;&#10;                parts = getattr(content, &quot;parts&quot;, []) if content else []&#13;&#10;                texts = []&#13;&#10;                for p in parts:&#13;&#10;                    t = getattr(p, &quot;text&quot;, None)&#13;&#10;                    if isinstance(t, str) and t:&#13;&#10;                        texts.append(t)&#13;&#10;                    elif isinstance(p, str) and p:&#13;&#10;                        texts.append(p)&#13;&#10;                answer = &quot;\n&quot;.join(texts)&#13;&#10;                finish_reason = getattr(first, &quot;finish_reason&quot;, None)&#13;&#10;                if not answer and finish_reason not in (None, 0):&#13;&#10;                    answer = (&#13;&#10;                        &quot;La respuesta fue bloqueada por las políticas de seguridad del modelo. &quot;&#13;&#10;                        &quot;Intenta reformular la pregunta con términos neutros y sin información sensible.&quot;&#13;&#10;                    )&#13;&#10;        except Exception:&#13;&#10;            pass&#13;&#10;        usage_md = getattr(response, &quot;usage_metadata&quot;, None)&#13;&#10;        usage = None&#13;&#10;        if usage_md:&#13;&#10;            # Best-effort conversion to dict&#13;&#10;            usage = {&#13;&#10;                k: getattr(usage_md, k)&#13;&#10;                for k in dir(usage_md)&#13;&#10;                if not k.startswith(&quot;_&quot;) and not callable(getattr(usage_md, k))&#13;&#10;            }&#13;&#10;        # If blocked or empty, try a single educational reframe to reduce safety triggers&#13;&#10;        if allow_reframe and (not answer or &quot;fue bloqueada&quot; in answer.lower() or finish_reason not in (None, 0)):&#13;&#10;            try:&#13;&#10;                re_user_prompt = (&#13;&#10;                    user_prompt&#13;&#10;                    + &quot;\n\nReformulación: Proporciona únicamente un resumen educativo general de alto nivel. &quot;&#13;&#10;                      &quot;Evita pasos operativos, cantidades, dosis, calendarios o imperativos. &quot;&#13;&#10;                      &quot;No incluyas productos, marcas ni instrucciones de ‘cómo hacer’. &quot;&#13;&#10;                      &quot;En su lugar, resume factores a considerar, buenas prácticas generales y señales de monitoreo, usando lenguaje condicional. &quot;&#13;&#10;                      &quot;No apliques límites de longitud estrictos; prioriza neutralidad y claridad (≈150–300 palabras en bullets).&quot;&#13;&#10;                )&#13;&#10;                response2 = self._model.generate_content(&#13;&#10;                    re_user_prompt,&#13;&#10;                    generation_config={**config, &quot;temperature&quot;: 0.1, &quot;top_p&quot;: 0.7},&#13;&#10;                    safety_settings=self._safety_settings(),&#13;&#10;                )&#13;&#10;                # Extract again&#13;&#10;                answer2 = &quot;&quot;&#13;&#10;                try:&#13;&#10;                    candidates2 = getattr(response2, &quot;candidates&quot;, None) or []&#13;&#10;                    if candidates2:&#13;&#10;                        first2 = candidates2[0]&#13;&#10;                        content2 = getattr(first2, &quot;content&quot;, None)&#13;&#10;                        parts2 = getattr(content2, &quot;parts&quot;, []) if content2 else []&#13;&#10;                        texts2 = []&#13;&#10;                        for p in parts2:&#13;&#10;                            t2 = getattr(p, &quot;text&quot;, None)&#13;&#10;                            if isinstance(t2, str) and t2:&#13;&#10;                                texts2.append(t2)&#13;&#10;                            elif isinstance(p, str) and p:&#13;&#10;                                texts2.append(p)&#13;&#10;                        answer2 = &quot;\n&quot;.join(texts2)&#13;&#10;                except Exception:&#13;&#10;                    pass&#13;&#10;                if answer2:&#13;&#10;                    answer = answer2&#13;&#10;            except Exception:&#13;&#10;                pass&#13;&#10;&#13;&#10;            # Second, more generic reframe if still empty/blocked&#13;&#10;            if not answer:&#13;&#10;                try:&#13;&#10;                    re_user_prompt2 = (&#13;&#10;                        &quot;Finalidad educativa: Ofrece un panorama general sobre manejo del agua en cultivos en términos amplios y neutros. &quot;&#13;&#10;                        &quot;Evita pasos operativos, cantidades, dosis, calendarios, marcas o productos. &quot;&#13;&#10;                        &quot;Usa bullets y lenguaje condicional para describir factores a considerar (clima, suelo, fenología, monitoreo), sin recomendaciones prescriptivas.&quot;&#13;&#10;                    )&#13;&#10;                    response3 = self._model.generate_content(&#13;&#10;                        re_user_prompt2,&#13;&#10;                        generation_config={**config, &quot;temperature&quot;: 0.1, &quot;top_p&quot;: 0.6},&#13;&#10;                        safety_settings=self._safety_settings(),&#13;&#10;                    )&#13;&#10;                    answer3 = &quot;&quot;&#13;&#10;                    try:&#13;&#10;                        candidates3 = getattr(response3, &quot;candidates&quot;, None) or []&#13;&#10;                        if candidates3:&#13;&#10;                            first3 = candidates3[0]&#13;&#10;                            content3 = getattr(first3, &quot;content&quot;, None)&#13;&#10;                            parts3 = getattr(content3, &quot;parts&quot;, []) if content3 else []&#13;&#10;                            texts3 = []&#13;&#10;                            for p in parts3:&#13;&#10;                                t3 = getattr(p, &quot;text&quot;, None)&#13;&#10;                                if isinstance(t3, str) and t3:&#13;&#10;                                    texts3.append(t3)&#13;&#10;                                elif isinstance(p, str) and p:&#13;&#10;                                    texts3.append(p)&#13;&#10;                            answer3 = &quot;\n&quot;.join(texts3)&#13;&#10;                    except Exception:&#13;&#10;                        pass&#13;&#10;                    if answer3:&#13;&#10;                        answer = answer3&#13;&#10;                except Exception:&#13;&#10;                    pass&#13;&#10;&#13;&#10;            # Third fallback: if requested short still fails, try concise-medium guidance&#13;&#10;            if not answer and (length == &quot;short&quot;):&#13;&#10;                try:&#13;&#10;                    re_user_prompt3 = (&#13;&#10;                        user_prompt&#13;&#10;                        + &quot;\n\nAjuste de formato: Responde de forma concisa (≈ 200–300 palabras) en bullets educativos. &quot;&#13;&#10;                          &quot;Evita pasos, cantidades numéricas, calendarios, marcas o productos. Usa lenguaje condicional.&quot;&#13;&#10;                    )&#13;&#10;                    response4 = self._model.generate_content(&#13;&#10;                        re_user_prompt3,&#13;&#10;                        generation_config=self._build_generation_config(length=&quot;medium&quot;),&#13;&#10;                        safety_settings=self._safety_settings(),&#13;&#10;                    )&#13;&#10;                    answer4 = &quot;&quot;&#13;&#10;                    try:&#13;&#10;                        candidates4 = getattr(response4, &quot;candidates&quot;, None) or []&#13;&#10;                        if candidates4:&#13;&#10;                            first4 = candidates4[0]&#13;&#10;                            content4 = getattr(first4, &quot;content&quot;, None)&#13;&#10;                            parts4 = getattr(content4, &quot;parts&quot;, []) if content4 else []&#13;&#10;                            texts4 = []&#13;&#10;                            for p in parts4:&#13;&#10;                                t4 = getattr(p, &quot;text&quot;, None)&#13;&#10;                                if isinstance(t4, str) and t4:&#13;&#10;                                    texts4.append(t4)&#13;&#10;                                elif isinstance(p, str) and p:&#13;&#10;                                    texts4.append(p)&#13;&#10;                            answer4 = &quot;\n&quot;.join(texts4)&#13;&#10;                    except Exception:&#13;&#10;                        pass&#13;&#10;                    if answer4:&#13;&#10;                        answer = answer4&#13;&#10;                except Exception:&#13;&#10;                    pass&#13;&#10;&#13;&#10;        return AskResponse(answer=answer.strip(), model=self.settings.gemini_model, usage=usage, tips=None)&#13;&#10;&#13;&#10;    def ask(self, req: AskRequest) -&gt; AskResponse:&#13;&#10;        if len(req.question) &gt; self.settings.max_input_chars:&#13;&#10;            raise ValueError(&quot;La pregunta es demasiado larga. Reduce el tamaño del texto.&quot;)&#13;&#10;&#13;&#10;        user_prompt = self._compose_user_prompt(req)&#13;&#10;        length = getattr(req, &quot;length&quot;, None) or &quot;medium&quot;&#13;&#10;&#13;&#10;        # If already in mock mode, return a deterministic demo response&#13;&#10;        if self.settings.mock_mode:&#13;&#10;            logger.debug(&quot;Using mock mode for response.&quot;)&#13;&#10;            tips = [&#13;&#10;                &quot;Incluye datos de suelo (pH, CE, % humedad) y clima (ET0, precipitación).&quot;,&#13;&#10;                &quot;Especifica el estado fenológico del cultivo para recomendaciones más precisas.&quot;,&#13;&#10;            ]&#13;&#10;            answer = (&#13;&#10;                &quot;[MODO DEMO] Recomendación preliminar para agricultura basada en la información disponible. &quot;&#13;&#10;                &quot;Agrega tu GEMINI_API_KEY en .env para respuestas reales.\n\n&quot;&#13;&#10;                f&quot;Resumen: {req.question[:180]}...\n\n&quot;&#13;&#10;                &quot;Siguiente paso: proporciona datos de suelo y clima para ajustar dosis y calendario.&quot;&#13;&#10;            )&#13;&#10;            return AskResponse(answer=answer, model=self.settings.gemini_model, usage=None, tips=tips)&#13;&#10;&#13;&#10;        # Ensure client is configured; on failure, configuration can toggle mock_mode&#13;&#10;        self._configure()&#13;&#10;        if self.settings.mock_mode:&#13;&#10;            # If configuration failed and switched to mock mode, return mock response now&#13;&#10;            logger.debug(&quot;Configuration switched to mock mode; returning demo response.&quot;)&#13;&#10;            tips = [&#13;&#10;                &quot;Incluye datos de suelo (pH, CE, % humedad) y clima (ET0, precipitación).&quot;,&#13;&#10;                &quot;Especifica el estado fenológico del cultivo para recomendaciones más precisas.&quot;,&#13;&#10;            ]&#13;&#10;            answer = (&#13;&#10;                &quot;[MODO DEMO] Recomendación preliminar para agricultura basada en la información disponible. &quot;&#13;&#10;                &quot;Agrega tu GEMINI_API_KEY en .env para respuestas reales.\n\n&quot;&#13;&#10;                f&quot;Resumen: {req.question[:180]}...\n\n&quot;&#13;&#10;                &quot;Siguiente paso: proporciona datos de suelo y clima para ajustar dosis y calendario.&quot;&#13;&#10;            )&#13;&#10;            return AskResponse(answer=answer, model=self.settings.gemini_model, usage=None, tips=tips)&#13;&#10;&#13;&#10;        return self._call_gemini(user_prompt, allow_reframe=bool(getattr(req, &quot;safe_mode&quot;, True)), length=length)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/vercel.json">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/vercel.json" />
              <option name="originalContent" value="&#10;&#10;&#10;" />
              <option name="updatedContent" value="{&#10;  &quot;version&quot;: 2,&#10;  &quot;routes&quot;: [&#10;    { &quot;src&quot;: &quot;/(.*)&quot;, &quot;dest&quot;: &quot;api/index.py&quot; }&#10;  ]&#10;}" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>